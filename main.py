import asyncio
from typing import AsyncIterable

from dotenv import load_dotenv, find_dotenv
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from langchain.callbacks import AsyncIteratorCallbackHandler
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from pydantic import BaseModel

load_dotenv(find_dotenv())

app  = FastAPI(
    title= "ðŸ¦œ | Streaming LangChain and FastAPI",
    version="0.0.1",
    description="A simple API server using Langchain runnables interfaces"
)

# Configurar CORS para permitir requisiÃ§Ãµes do frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Ou especifique ["http://localhost:5500"] se precisar de mais seguranÃ§a
    allow_credentials=True,
    allow_methods=["*"],  # Permite todos os mÃ©todos (GET, POST, OPTIONS, etc.)
    allow_headers=["*"],  # Permite todos os headers
)

class Message(BaseModel):
    content: str

async def send_message(content: str) -> AsyncIterable[str]:
    callback = AsyncIteratorCallbackHandler()
    model = ChatOpenAI(
        streaming=True,
        verbose=True,
        callbacks=[callback],
    )

    task = asyncio.create_task(
        model.agenerate(messages=[[HumanMessage(content=content)]])
    )

    try:
        async for token in callback.aiter():
            yield token
    except Exception as e:
        print(f"Caught exception: {e}")
    finally:
        callback.done.set()
    
    await task

@app.post("/stream_chat/")
async def stream_chat(message: Message):
    generator = send_message(message.content)
    return StreamingResponse(generator, media_type="text/event-stream")